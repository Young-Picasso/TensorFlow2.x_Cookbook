{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a92efce",
   "metadata": {},
   "source": [
    "## 1. Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21b5baab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-18 08:36:40.087434: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-01-18 08:36:41.108001: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-01-18 08:36:41.150785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 08:36:41.151148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:2d:00.0 name: GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.785GHz coreCount: 40 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2022-01-18 08:36:41.151161: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-01-18 08:36:41.152722: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-01-18 08:36:41.152748: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-01-18 08:36:41.153342: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-18 08:36:41.153520: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-18 08:36:41.155032: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-01-18 08:36:41.155411: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-01-18 08:36:41.155544: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-01-18 08:36:41.155652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 08:36:41.156307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 08:36:41.156637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2022-01-18 08:36:41.156922: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-18 08:36:41.157449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 08:36:41.157789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:2d:00.0 name: GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.785GHz coreCount: 40 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2022-01-18 08:36:41.157827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 08:36:41.158175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 08:36:41.158493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2022-01-18 08:36:41.158511: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-01-18 08:36:41.488219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-01-18 08:36:41.488241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2022-01-18 08:36:41.488245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2022-01-18 08:36:41.488391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 08:36:41.488744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 08:36:41.489072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-18 08:36:41.489382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6282 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:2d:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "iris = tfds.load(\"iris\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab6047",
   "metadata": {},
   "source": [
    "## 2. Birth weight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e78fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d77bcc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/PacktPublishing/TensorFlow-2-Machine-Learning-Cookbook-Third-Edition/master/birthweight.dat\n",
      "8192/4554 [=====================================================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "birthdata_url = 'https://raw.githubusercontent.com/PacktPublishing/TensorFlow-2-Machine-Learning-Cookbook-Third-Edition/master/birthweight.dat'\n",
    "path = tf.keras.utils.get_file(birthdata_url.split('/')[-1], birthdata_url)\n",
    "\n",
    "def map_line(x):\n",
    "    return tf.strings.to_number(tf.strings.split(x))\n",
    "\n",
    "birth_file = (tf.data.TextLineDataset(path).skip(1).map(map_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130aed0f",
   "metadata": {},
   "source": [
    "## 3. Boston housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1709e6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\n",
      "49152/49082 [==============================] - 0s 3us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "housing_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\n",
    "path = tf.keras.utils.get_file(housing_url.split('/')[-1], housing_url)\n",
    "\n",
    "def map_line(x):\n",
    "    return tf.strings.to_number(tf.strings.split(x))\n",
    "\n",
    "housing = (tf.data.TextLineDataset(path).map(map_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fedae3",
   "metadata": {},
   "source": [
    "## 4. MNIST handwriting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69a68f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-18 08:44:19.473188: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Failed precondition: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /home/wil/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cad5148a4b4b3abac6610291c92add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...:   0%|          | 0/4 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /home/wil/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "mnist = tfds.load('mnist', split=None)\n",
    "mnist_train = mnist['train']\n",
    "mnist_test = mnist['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c9527",
   "metadata": {},
   "source": [
    "## 5. Spam-ham text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac2d1ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
      "204800/203415 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "path = tf.keras.utils.get_file(zip_url.split('/')[-1], zip_url, extract=True)\n",
    "\n",
    "path = path.replace(\"smsspamcollection.zip\", \"SMSSpamCollection\")\n",
    "\n",
    "def split_text(x):\n",
    "    return tf.strings.split(x, sep='\\t')\n",
    "\n",
    "text_data = (tf.data.TextLineDataset(path).map(split_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e258c9",
   "metadata": {},
   "source": [
    "## 6. Movie review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4abc7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
      "491520/487770 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12765/596997136.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.tar.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmovie_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mresponce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'\\\\rt-polarity.neg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\\\rt-polarity.pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmovie_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "path = tf.keras.utils.get_file(movie_data_url.split('/')[-1], movie_data_url, extract=True)\n",
    "\n",
    "path = path.replace('.tar.gz', '')\n",
    "\n",
    "with open(path+filename, 'r', encoding='utf-8', errors='ignore') as movie_file:\n",
    "    for responce, filename in enumerate(['\\\\rt-polarity.neg', '\\\\rt-polarity.pos']):\n",
    "        with open(path+filename, 'r') as movie_file:\n",
    "            for line in movie_file:\n",
    "                review_file.write(str(response) + '\\t' + line.encode('utf-8').decode())\n",
    "                \n",
    "def split_text(x):\n",
    "         return tf.strings.split(x, sep='\\t')\n",
    "\n",
    "movies = (tf.data.TextLineDataset('movie_reviews.txt').map(split_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe68235",
   "metadata": {},
   "source": [
    "## 7. CIFAR-10 image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb43bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 162.17 MiB (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB) to /home/wil/tensorflow_datasets/cifar10/3.0.2...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e056fe261521474dba475a8a1e402a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d45eb2cf0847fcbdf93e3b5182e193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51813054753b4e43867b3c77b1813d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling cifar10-train.tfrecord...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling cifar10-test.tfrecord...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset cifar10 downloaded and prepared to /home/wil/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\u001b[0m\n",
      "tfds.core.DatasetInfo(\n",
      "    name='cifar10',\n",
      "    full_name='cifar10/3.0.2',\n",
      "    description=\"\"\"\n",
      "    The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
      "    \"\"\",\n",
      "    homepage='https://www.cs.toronto.edu/~kriz/cifar.html',\n",
      "    data_path='/home/wil/tensorflow_datasets/cifar10/3.0.2',\n",
      "    download_size=162.17 MiB,\n",
      "    dataset_size=132.40 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'id': Text(shape=(), dtype=tf.string),\n",
      "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
      "        author = {Alex Krizhevsky},\n",
      "        title = {Learning multiple layers of features from tiny images},\n",
      "        institution = {},\n",
      "        year = {2009}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds, info = tfds.load('cifar10', shuffle_files=True, with_info=True)\n",
    "\n",
    "print(info)\n",
    "\n",
    "cifar_train = ds['train']\n",
    "cifar_test = ds['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05cd47",
   "metadata": {},
   "source": [
    "## 8. The works of Shakespeare text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "794e2b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/PacktPublishing/TensorFlow-2-Machine-Learning-Cookbook-Third-Edition/master/shakespeare.txt\n",
      "5472256/5465102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "shakespeare_url = 'https://raw.githubusercontent.com/PacktPublishing/TensorFlow-2-Machine-Learning-Cookbook-Third-Edition/master/shakespeare.txt'\n",
    "path = tf.keras.utils.get_file(shakespeare_url.split('/')[-1], shakespeare_url)\n",
    "\n",
    "def split_text(x):\n",
    "    return tf.strings.split(x, sep='\\n')\n",
    "\n",
    "shakespeare_text = (tf.data.TextLineDataset(path).map(split_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7add14",
   "metadata": {},
   "source": [
    "## 9. English-German sentence translation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "446c9215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen, Request\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "sentence_url = 'https://www.manythings.org/anki/deu-eng.zip'\n",
    "\n",
    "r = Request(sentence_url, headers={'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11'})\n",
    "b2 = [z for z in sentence_url.split('/') if '.zip' in z][0] # gets just the '.zip' part of the url\n",
    "\n",
    "with open(b2, \"wb\") as target:\n",
    "    target.write(urlopen(r).read()) # saves to file to disk\n",
    "    \n",
    "with ZipFile(b2) as z:\n",
    "    deu = [line.split('\\t')[:2] for line in z.open('deu.txt').read().decode().split('\\n')]\n",
    "    \n",
    "os.remove(b2) #removes the zip file\n",
    "\n",
    "# saving to disk prepared en-de sentence file\n",
    "with open ('deu.txt', 'wb') as deu_file:\n",
    "    for line in deu:\n",
    "        data = ','.join(line)+'\\n'\n",
    "        deu_file.write(data.encode('utf-8'))\n",
    "        \n",
    "def split_text(x):\n",
    "    return tf.strings.split(x, sep=',')\n",
    "\n",
    "text_data = (tf.data.TextLineDataset(\"deu.txt\").map(split_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a29c8",
   "metadata": {},
   "source": [
    "## How it works..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04336c91",
   "metadata": {},
   "source": [
    "When it comes to using one of these datasets in a recipe, we'll refer you to this section and assume that the data is loaded in the ways we've just described. If further data transformation or preprocessing is necessary, then that code will be provided in the recipe itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4fd422",
   "metadata": {},
   "source": [
    "Usually, the appraoch will simply be as follows when we use data from TensorFlow datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset_name = \"...\"\n",
    "data = tfds.load(dataset_name, split=None)\n",
    "train = data['train']\n",
    "test = data['test']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
