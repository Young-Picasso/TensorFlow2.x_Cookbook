{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import Transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8c7cf02d034ee39d725ae07fc44e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04f00c49c6c4914abf428845bafd83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53fe7901b2d451abeca39976e3bbc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a3a6ebe001438baf6eb9bc81ca83e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f8750010ff42e4962c81b7b782a6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 34\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# set maximum number of words in output text\n",
    "MAX_LEN = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Using greedy search, the word with the highest probability is predicted as the next word in the sequence. We can observe how this works in the below code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = 'There are times when I am really tired of people, but I feel lonely too.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an input sequence, we can encode it and call a decode method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are times when I am really tired of people, but I feel lonely too. I feel like I'm alone in the world. I feel like I'm alone in my own body. I feel like I'm alone in my own mind. I feel like I'm alone in my own heart. I feel like I'm alone in my own mind\n"
     ]
    }
   ],
   "source": [
    "# Encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n",
    "\n",
    "# generate test until the output length (which includes the context length) reaches 70\n",
    "greedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that this is repetitive and not ideal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Beam search offers a solution, in beam search one tracks the alternative variants, so that more comparisons are possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
      "\n",
      "\"I feel like I can't do anything right now,\" she said. \"I'm so tired.\"\n",
      "1: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
      "\n",
      "\"I feel like I can't do anything right now,\" she says. \"I'm so tired.\"\n",
      "2: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
      "\n",
      "\"I feel like I can't do anything right now,\" she says. \"I'm not sure what I'm supposed to be doing with my life.\"\n",
      "3: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
      "\n",
      "\"I feel like I can't do anything right now,\" she says. \"I'm not sure what I'm supposed to be doing.\"\n",
      "4: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
      "\n",
      "\"I feel like I can't do anything right now,\" she says. \"I'm not sure what I should do.\"\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = GPT2.generate(input_ids, \n",
    "                             max_length=MAX_LEN,\n",
    "                             num_beams = 5,\n",
    "                             no_repeat_ngram_size = 2,\n",
    "                             num_return_sequences = 5,\n",
    "                             early_stopping= True)\n",
    "\n",
    "print('')\n",
    "print('Output:\\n' + 100 * '-')\n",
    "\n",
    "# We now have 5 output sequences\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "    print(f'{i}: {tokenizer.decode(beam_output, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides a more diverse response, even though the messages tend to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We can now explore sampling - indeterministic decoding. So instead of following a strict path to find the end text with the highest probability, we rather randomly pick the next word by it's conditional probability distribution. A draw back to this is the risk of incoherent ramblings, so a temperature parameter is used to affect the probability mass distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are times when I am really tired of people, but I feel lonely too. I feel like I'm alone in my own world. I feel like I'm alone in my own life. I feel like I'm alone in my own mind. I feel like I'm alone in my own heart. I feel like I'm alone in my own\n"
     ]
    }
   ],
   "source": [
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = GPT2.generate(input_ids,\n",
    "                              do_sample = True,\n",
    "                              max_length = MAX_LEN,\n",
    "                              top_k = 0,\n",
    "                              temperature = 0.2)\n",
    "\n",
    "print('Output:\\n' + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one can be labelled the \"senti\" model, because it is clearly in its feels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Observe the differences in raising the temperature now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are times when I am really tired of people, but I feel lonely too. I find it strange how the people around me seem to be always so nice. The only time I feel lonely is when I'm on the road. I can't be alone with my thoughts.\n",
      "\n",
      "What are some of your favourite things to do in the area\n"
     ]
    }
   ],
   "source": [
    "sample_output = GPT2.generate(input_ids,\n",
    "                              do_sample = True,\n",
    "                              max_length = MAX_LEN,\n",
    "                              top_k = 0,\n",
    "                              temperature = 0.8)\n",
    "\n",
    "print('Output:\\n' + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a stark difference, but feels like a train of thought. Beyond temperature, there are more ways to tune the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. In Top-K sampling the top k most likely next words are selected and the entire probability mass is shifted to these k words. This removes the low probablity words from occuring altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are times when I am really tired of people, but I feel lonely too. I go to a place where you can feel comfortable. It's a place where you can relax. But if you're so tired of going along with the rules, maybe I won't go. You know what? Maybe if I don't go, you won't\n"
     ]
    }
   ],
   "source": [
    "# sample from only top_k most likely words\n",
    "sample_output = GPT2.generate(input_ids,\n",
    "                              do_sample = True,\n",
    "                              max_length = MAX_LEN,\n",
    "                              top_k = 50)\n",
    "\n",
    "print('Output:\\n' + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is beginning to look better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Top-P sampling (nucleus sampling) is similar to Top-K, but instead of choosing the top-k most likely words, we choose the smallest set of words whose total probablity is larger than p, and then the entire probability mass is shifted to the words in this set. In top-p the size of words is dynamic where as in top-k is it static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are times when I am really tired of people, but I feel lonely too. I feel like I should just be standing there, just sitting there. I know I'm not a danger to anybody. I just feel alone.\"\n"
     ]
    }
   ],
   "source": [
    "# sample from only top_k most likely words\n",
    "sample_output = GPT2.generate(input_ids,\n",
    "                              do_sample = True,\n",
    "                              max_length = MAX_LEN,\n",
    "                              top_p = 0.8,\n",
    "                              top_k = 0)\n",
    "\n",
    "print('Output:\\n' + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idk, kinda sus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. We can also combine these approaches and really let the model speak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: There are times when I am really tired of people, but I feel lonely too. I feel like, even though I don't have any friends, I'm still the same person that I am and I have been for 20 years. But I feel lonely sometimes. But I do get to have friends now and then. I still haven't gotten to have a real boyfriend.\"\n",
      "\n",
      "While she didn't want to talk about her sex life, Kim also shared her opinions about being single, being single in a relationship, and having a child. \"I do feel a lot of guilt,\" she explained. \"But I also feel very happy and lucky and grateful that my life is so interesting.\"\n",
      "\n",
      "1: There are times when I am really tired of people, but I feel lonely too. You don't have to be in love with me, you can just be in love with someone else, even if it's a friend.\"\n",
      "\n",
      "Her boyfriend, whom she calls \"Mr. Big\" — his height, size, height and weight are similar to hers — doesn't mind the fact that she's with someone else; he wants her to be happy.\n",
      "\n",
      "She likes that people don't know he's her boyfriend. She doesn't mind at all when people call her \"Mrs. Big.\" But sometimes, she wishes someone would be more aware of her relationship with her boyfriend and realize that she\n",
      "2: There are times when I am really tired of people, but I feel lonely too. I feel like I'm in this world where I'm just alone and that's the only way I can feel that I'm not alone. I can't be alone. I want to be loved, to feel loved. I want to feel safe and I want to feel loved. I want to be loved by other people, but they're not going to be there. That's all I have, and that's why I want to be alone.\n",
      "\n",
      "This is why I wanted to be alone.\n",
      "\n",
      "It's why I felt the need to leave, to be a stray.\n",
      "\n",
      "I've been\n",
      "3: There are times when I am really tired of people, but I feel lonely too. There is a lot of good music, but the most memorable is \"Mariachi Chicka Chicka\". I love the way the drums and bass work together. I love the way the vocals are different from the music. I like it when they sing a song about me, or that I am someone special.\n",
      "\n",
      "I like a lot of things in the world and I love the way I see the world. I can't really understand how it happened, but it's not really that important. I just like to see, and I want to help people see. I'm not the only one like this\n",
      "4: There are times when I am really tired of people, but I feel lonely too. It was like, I'm doing this, I want to do it, but there are other people doing it for me. I wanted to be alone with the pain.\n",
      "\n",
      "And I wanted to talk to him about this. I wanted to explain it to him. I didn't want to be in his shoes. I didn't want to be like, \"I can't even feel this pain, and now I want to do it myself.\" I wanted to be like, \"You're not alone, but you're not alone in pain, either. It's normal.\" It's just a way for me\n"
     ]
    }
   ],
   "source": [
    "# sample from only top_k most likely words\n",
    "sample_outputs = GPT2.generate(input_ids,\n",
    "                              do_sample = True,\n",
    "                              max_length = 2*MAX_LEN, # to test how long we can generate coherent data\n",
    "                              top_p = 0.85,\n",
    "                              top_k = 50,\n",
    "                              num_return_sequences = 5)\n",
    "\n",
    "# We now have 5 output sequences\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f'{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking alright, one can reasonably follow the core idea here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. We can use prompts directly from OpenAI's GPT-2 website and compare our results with a smaller local model to the full sized GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\"We didn't know that there were unicorns in this valley. We don't have any evidence that they speak English. But the people of this valley are extremely friendly and they gave us a lot of information about the local language,\" said Dr. David Cramer, the lead researcher of the research at the University of British Columbia in Canada.\n",
      "\n",
      "\"The way the unicorn spoke was very different from that of any other animals,\" he added. \"They were very fast, very intelligent. And they were very, very gentle. They were very good with their hooves. They were very gentle. They would go up and hold on to us when we were walking down the street.\"\n",
      "\n",
      "The researchers' next challenge was to find the unicorn herd. The valley was in Argentina, which is not very far from Chile, where a herd of about 40 unicorns has been spotted. Cramer and his colleagues are hoping to eventually catch up to them.\n",
      "\n",
      "\"The fact that these unicorns speak English means that they can communicate in a way that's very close to the way that they speak to us,\" Cramer said. \"So we've got very good odds of finding them.\"\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 500\n",
    "\n",
    "prompt1 = 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt1, return_tensors='tf')\n",
    "\n",
    "sample_outputs = GPT2.generate(input_ids,\n",
    "                               do_sample = True,\n",
    "                               max_length = MAX_LEN,\n",
    "                               top_k = 50,\n",
    "                               top_p = 0.85)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f'{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. It is also possible to generate fake news with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today. She was released on $7,500 bail with conditions of not drinking alcohol, not using drugs, and not committing a crime. (Photo: Courtesy/ABC7)\n",
      "\n",
      "Hollywood Boulevard was closed off at Westwood Boulevard for more than an hour as police investigated the theft of designer clothing.\n",
      "\n",
      "Police said a vehicle pulled over near Highland Boulevard and Beverly Boulevard and the suspect was arrested on suspicion of shoplifting a $7,500 watch and $12,000 in other designer clothes.\n",
      "\n",
      "No arrests have been made in the case.\n",
      "\n",
      "Hollywood Boulevard was closed for more than an hour due to the arrest of a woman on suspicion of shoplifting. (Photo: Courtesy/ABC7)\n",
      "\n",
      "The incident happened just before noon at the store, located at 9400 Westwood Boulevard.\n",
      "\n",
      "The store's hours are 6 a.m. to 11 p.m.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 500\n",
    "\n",
    "prompt2 = 'Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt2, return_tensors='tf')\n",
    "\n",
    "sample_outputs = GPT2.generate(input_ids,\n",
    "                               do_sample = True,\n",
    "                               max_length = MAX_LEN,\n",
    "                               top_k = 50,\n",
    "                               top_p = 0.85)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f'{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. What about some Lord of the Rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      "\n",
      "\"Let us finish this,\" shouted Gimli, \"And then we will come to Gondor.\"\n",
      "\n",
      "And so they did, and Gimli led his brother and the Fellowship to Rivendell, where Frodo and Sam reached their camp on the banks of the river.\n",
      "\n",
      "It was time for the rest of the Fellowship to join them, and they had gathered in the tent. The rest of the party had gathered around it as well, for they had been ordered to remain in the camp.\n",
      "\n",
      "\"We must come to our home soon,\" said Elrond. \"There are great things to see, and to do.\"\n",
      "\n",
      "\"Then,\" said Gimli, \"let us meet at the gate, at the Gate of Gondor, so that we may come and go as we will.\"\n",
      "\n",
      "\"And I?\" asked Sam, but Elrond shook his head. \"I will not go on this journey.\"\n",
      "\n",
      "\"Not until I have done so,\" said Elrond, \"because it is better for you than for me.\"\n",
      "\n",
      "\"Do you love the land of Gondor,\" said Gimli, \"or do you not?\"\n",
      "\n",
      "\"I have many memories of Gondor,\" said Gimli, \"but not all of them.\"\n",
      "\n",
      "\"How long have you known of it?\" asked Elrond, \"and of the great deeds that have been done in it?\"\n",
      "\n",
      "\"It is far too early for me to tell,\" said Gimli. \"If you wish to learn what I have been doing all these years, I would ask only that you know of my father's service to Middle-earth. I was born with him.\"\n",
      "\n",
      "\"Gandalf,\" said Elrond, \"you must tell us more. Do you remember your father's service to Middle-earth? I will tell you. He was at the beginning of the War of the Ring, and with him went the three Rings of Power, and the First and the Last.\"\n",
      "\n",
      "\"And you,\" said Gimli, \"can tell us of all these things?\"\n",
      "\n",
      "\"Of course,\" said Elrond, \"but only if you have seen and known them.\"\n",
      "\n",
      "\"What are the Rings of Power?\" asked Gimli.\n",
      "\n",
      "\"I do not know,\" said El\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 500\n",
    "\n",
    "prompt2 = 'Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt2, return_tensors='tf')\n",
    "\n",
    "sample_outputs = GPT2.generate(input_ids,\n",
    "                               do_sample = True,\n",
    "                               max_length = MAX_LEN,\n",
    "                               top_k = 50,\n",
    "                               top_p = 0.85)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f'{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool stuff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py365",
   "language": "python",
   "name": "py365"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
