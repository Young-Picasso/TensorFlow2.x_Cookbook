{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_agents.environments import py_environment, tf_environment, tf_py_environment, utils, wrappers, suite_gym\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import trajectory, time_step as ts\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.metrics import tf_metrics, py_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.drivers import py_driver, dynamic_episode_driver\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Setup the environment for the agent to act within, and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(py_environment.PyEnvironment):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=3, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(4,), dtype=np.int32, minimum=[0,0,0,0], maximum=[5,5,5,5], name='observation')\n",
    "        self._state=[0,0,5,5] # Represents the (row, col, frow, fcol) of the player and finish\n",
    "        self._episode_ended = False\n",
    "    \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    # Reset the state once finished\n",
    "    def _reset(self):\n",
    "        self._state=[0,0,5,5]\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "    \n",
    "    # Step function handles the state transition by applying an action to the current state to obtain a new one\n",
    "    def _step(self, action):\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "        \n",
    "        self.move(action)\n",
    "        \n",
    "        if self.game_over():\n",
    "            self._episode_ended = True\n",
    "            \n",
    "        if self._episode_ended:\n",
    "            if self.game_over():\n",
    "                reward = 100\n",
    "            else:\n",
    "                reward = 0\n",
    "            return ts.termination(np.array(self._state, dtype=np.int32), reward)\n",
    "        else:\n",
    "            return ts.transition(\n",
    "                np.array(self._state, dtype=np.int32), reward=0, discount=0.9)\n",
    "        \n",
    "    def move(self, action):\n",
    "        row, col, frow, fcol = self._state[0], self._state[1], self._state[2], self._state[3]\n",
    "        if action == 0: # down\n",
    "            if row - 1 > 0:\n",
    "                self._state[0] -= 1\n",
    "        if action == 1: # up\n",
    "            if row + 1 < 6:\n",
    "                self._state[0] += 1\n",
    "        if action == 2: # left\n",
    "            if col - 1 > 0:\n",
    "                self._state[1] -= 1\n",
    "        if action == 3: # right\n",
    "            if col + 1 < 6:\n",
    "                self._state[1] += 1\n",
    "                \n",
    "    def game_over(self):\n",
    "        row, col, frow, fcol = self._state[0], self._state[1], self._state[2], self._state[3]\n",
    "        return row==frow and col==fcol\n",
    "    \n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    \n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        \n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        \n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            total_return += episode_return\n",
    "            \n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_Step, next_time_step)\n",
    "    \n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Setup preliminary conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter settings\n",
    "\n",
    "num_iterations = 10_000\n",
    "initial_collection_steps = 1_000\n",
    "collect_steps_per_iteration = 1\n",
    "replay_buffer_capacity = 100_000\n",
    "fc_layer_params = (100,)\n",
    "batch_size = 128\n",
    "learning_rate = 1e-5\n",
    "log_interval = 200\n",
    "num_eval_episodes = 2\n",
    "eval_interval = 1_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Start by creating the enviroments and wrapping them to ensure termination after 100 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = wrappers.TimeLimit(GridWorldEnv(), duration=100)\n",
    "eval_py_env = wrappers.TimeLimit(GridWorldEnv(), duration=100)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We use a Deep Q-Network (DQN) agent for this recipe. As such, we will have to define the network and associated optimizer first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = q_network.QNetwork(train_env.observation_spec(),\n",
    "                           train_env.action_spec(),\n",
    "                           fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Backward compatibility is required here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.compat.v2.Variable(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Define the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent = dqn_agent.DqnAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        q_network=q_net,\n",
    "        optimizer=optimizer,\n",
    "        td_errors_loss_fn = common.element_wise_squared_loss,\n",
    "        train_step_counter=train_step_counter)\n",
    "\n",
    "tf_agent.initialize()\n",
    "\n",
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create replay buffer and replay observer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 1\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(data_spec=tf_agent.collect_data_spec,\n",
    "                                                               batch_size = train_env.batch_size,\n",
    "                                                               max_length = replay_buffer_capacity)\n",
    "\n",
    "print(f'Batch Size: {train_env.batch_size}')\n",
    "\n",
    "replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Create a dataset from buffer so that it can be iterated over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(num_parallel_calls=3,\n",
    "                                   sample_batch_size=batch_size,\n",
    "                                   num_steps=2).prefetch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a driver that will simulate the agent in the game and store the (state, action, reward) tuples in the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "driver = dynamic_step_driver.DynamicStepDriver(train_env,\n",
    "                                               collect_policy,\n",
    "                                               observers=replay_observer + train_metrics,\n",
    "                                               num_steps=1)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "print(compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes))\n",
    "\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "final_time_step, policy_state = driver.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Run the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wil/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "step = 200: loss = 0.027144549414515495\n",
      "Average episode length: 100.0\n",
      "step = 400: loss = 0.019206177443265915\n",
      "Average episode length: 100.0\n",
      "step = 600: loss = 0.029454940930008888\n",
      "Average episode length: 74.0\n",
      "step = 800: loss = 75.94647979736328\n",
      "Average episode length: 22.5\n",
      "step = 1000: loss = 152.3343048095703\n",
      "Average episode length: 36.099998474121094\n",
      "step = 1000: Average return = 0.0\n",
      "step = 1200: loss = 224.23692321777344\n",
      "Average episode length: 50.70000076293945\n",
      "step = 1400: loss = 149.57388305664062\n",
      "Average episode length: 60.099998474121094\n",
      "step = 1600: loss = 74.2284927368164\n",
      "Average episode length: 53.20000076293945\n",
      "step = 1800: loss = 292.94647216796875\n",
      "Average episode length: 42.599998474121094\n",
      "step = 2000: loss = 0.07912465929985046\n",
      "Average episode length: 36.099998474121094\n",
      "step = 2000: Average return = 0.0\n",
      "step = 2200: loss = 71.7882308959961\n",
      "Average episode length: 39.400001525878906\n",
      "step = 2400: loss = 214.630859375\n",
      "Average episode length: 34.20000076293945\n",
      "step = 2600: loss = 70.60731506347656\n",
      "Average episode length: 14.100000381469727\n",
      "step = 2800: loss = 278.88299560546875\n",
      "Average episode length: 23.399999618530273\n",
      "step = 3000: loss = 414.7958068847656\n",
      "Average episode length: 10.600000381469727\n",
      "step = 3000: Average return = 0.0\n",
      "step = 3200: loss = 203.71815490722656\n",
      "Average episode length: 29.200000762939453\n",
      "step = 3400: loss = 269.45281982421875\n",
      "Average episode length: 19.899999618530273\n",
      "step = 3600: loss = 398.39691162109375\n",
      "Average episode length: 11.199999809265137\n",
      "step = 3800: loss = 131.59762573242188\n",
      "Average episode length: 11.199999809265137\n",
      "step = 4000: loss = 324.1118469238281\n",
      "Average episode length: 10.600000381469727\n",
      "step = 4000: Average return = 100.0\n",
      "step = 4200: loss = 127.37224578857422\n",
      "Average episode length: 10.800000190734863\n",
      "step = 4400: loss = 314.78778076171875\n",
      "Average episode length: 11.0\n",
      "step = 4600: loss = 435.3880920410156\n",
      "Average episode length: 10.600000381469727\n",
      "step = 4800: loss = 246.60745239257812\n",
      "Average episode length: 12.0\n",
      "step = 5000: loss = 423.83660888671875\n",
      "Average episode length: 11.399999618530273\n",
      "step = 5000: Average return = 100.0\n",
      "step = 5200: loss = 415.4015808105469\n",
      "Average episode length: 11.100000381469727\n",
      "step = 5400: loss = 290.4952087402344\n",
      "Average episode length: 10.800000190734863\n",
      "step = 5600: loss = 173.12025451660156\n",
      "Average episode length: 10.899999618530273\n",
      "step = 5800: loss = 282.25921630859375\n",
      "Average episode length: 10.699999809265137\n",
      "step = 6000: loss = 555.9784545898438\n",
      "Average episode length: 10.699999809265137\n",
      "step = 6000: Average return = 100.0\n",
      "step = 6200: loss = 163.84661865234375\n",
      "Average episode length: 10.399999618530273\n",
      "step = 6400: loss = 644.4423828125\n",
      "Average episode length: 11.399999618530273\n",
      "step = 6600: loss = 422.45599365234375\n",
      "Average episode length: 10.899999618530273\n",
      "step = 6800: loss = 310.56634521484375\n",
      "Average episode length: 11.5\n",
      "step = 7000: loss = 356.98095703125\n",
      "Average episode length: 10.800000190734863\n",
      "step = 7000: Average return = 100.0\n",
      "step = 7200: loss = 451.3409423828125\n",
      "Average episode length: 10.600000381469727\n",
      "step = 7400: loss = 248.41064453125\n",
      "Average episode length: 10.600000381469727\n",
      "step = 7600: loss = 192.65267944335938\n",
      "Average episode length: 10.5\n",
      "step = 7800: loss = 284.0771179199219\n",
      "Average episode length: 11.699999809265137\n",
      "step = 8000: loss = 278.9256591796875\n",
      "Average episode length: 10.699999809265137\n",
      "step = 8000: Average return = 100.0\n",
      "step = 8200: loss = 549.7863159179688\n",
      "Average episode length: 10.800000190734863\n",
      "step = 8400: loss = 448.44921875\n",
      "Average episode length: 10.800000190734863\n",
      "step = 8600: loss = 352.16357421875\n",
      "Average episode length: 11.199999809265137\n",
      "step = 8800: loss = 554.01513671875\n",
      "Average episode length: 10.600000381469727\n",
      "step = 9000: loss = 548.7440795898438\n",
      "Average episode length: 11.399999618530273\n",
      "step = 9000: Average return = 100.0\n",
      "step = 9200: loss = 292.2604064941406\n",
      "Average episode length: 11.100000381469727\n",
      "step = 9400: loss = 202.85205078125\n",
      "Average episode length: 11.5\n",
      "step = 9600: loss = 354.64398193359375\n",
      "Average episode length: 10.899999618530273\n",
      "step = 9800: loss = 310.5300598144531\n",
      "Average episode length: 11.300000190734863\n",
      "step = 10000: loss = 266.473876953125\n",
      "Average episode length: 10.899999618530273\n",
      "step = 10000: Average return = 100.0\n"
     ]
    }
   ],
   "source": [
    "episode_len = []\n",
    "step_len = []\n",
    "for i in range(num_iterations):\n",
    "    final_time_step, _ = driver.run(final_time_step, policy_state)\n",
    "    \n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = tf_agent.train(experience=experience)\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print(f'step = {step}: loss = {train_loss.loss}')\n",
    "        episode_len.append(train_metrics[3].result().numpy())\n",
    "        step_len.append(step)\n",
    "        print(f'Average episode length: {train_metrics[3].result().numpy()}')\n",
    "        \n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "        print(f'step = {step}: Average return = {avg_return}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Visualize the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(step_len, episode_len)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Episode Length (Steps)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see how the agent learns an ideal policy to be able to navigate to the desired state in the least number of episodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
