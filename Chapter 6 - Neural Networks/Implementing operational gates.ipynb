{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ready..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create one gate, f(x) = a\\*x. Then we will add a second for f(x) = a\\*x + b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Declare our model variable and input data. Make the input data equal to value 5 so that multiplication\n",
    "factor to get 50 will be 10.\"\"\"\n",
    "a = tf.Variable(4.)\n",
    "x_data = tf.keras.Input(shape=(1,))\n",
    "x_val = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=() dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Create a lambda layer that computes the operation, and create a functional Keras model with following\n",
    "input:\"\"\"\n",
    "multiply_layer = tf.keras.layers.Lambda(lambda x:tf.multiply(a, x))\n",
    "outputs = multiply_layer(x_data)\n",
    "model = tf.keras.Model(inputs=x_data, outputs=outputs, name='gate_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Declare optimizing algorithm as stochastic gradient descent as follows:\"\"\"\n",
    "optimizer = tf.keras.optimizers.SGD(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing a Multiplication Gate Output to 50.\n",
      "7.0 * 5 = 35.0\n",
      "8.5 * 5 = 42.5\n",
      "9.25 * 5 = 46.25\n",
      "9.625 * 5 = 48.125\n",
      "9.8125 * 5 = 49.0625\n",
      "9.90625 * 5 = 49.53125\n",
      "9.953125 * 5 = 49.765625\n",
      "9.9765625 * 5 = 49.8828125\n",
      "9.98828125 * 5 = 49.94140625\n",
      "9.994140625 * 5 = 49.970703125\n"
     ]
    }
   ],
   "source": [
    "\"\"\" We can now optimize our model output\"\"\"\n",
    "print('Optimizing a Multiplication Gate Output to 50.')\n",
    "for i in range (10):\n",
    "    \n",
    "    # Open a GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Forward pass\n",
    "        mult_output = model(x_val)\n",
    "        \n",
    "        # Loss value as the difference between the output and target value, 50\n",
    "        loss_value = tf.square(tf.subtract(mult_output, 50.))\n",
    "        \n",
    "    # Get gradients of loss with reference to the variable \"a\" to adjust\n",
    "    gradients = tape.gradient(loss_value, a)\n",
    "    \n",
    "    # Update the variable \"a\" of the model.\n",
    "    optimizer.apply_gradients(zip([gradients], [a]))\n",
    "    \n",
    "    print(f'{a.numpy()} * {x_val} = {a.numpy()*x_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with two-nested gates for f(x) = a*x + b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=() dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_2), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=() dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" We start in exactly the same way as the preeceding example, but will initialize two model variables,\n",
    "a and b, as follows:\"\"\"\n",
    "x_data = tf.keras.Input(dtype=tf.float32, shape=(1,))\n",
    "x_val = 5.\n",
    "a = tf.Variable(1., dtype=tf.float32)\n",
    "b = tf.Variable(1., dtype=tf.float32)\n",
    "\n",
    "# Add a layer which computes f(x) = a * x\n",
    "multiply_layer = tf.keras.layers.Lambda(lambda x: tf.multiply(a, x))\n",
    "\n",
    "# Add a layer which computes f(x) = b + x\n",
    "add_layer = tf.keras.layers.Lambda(lambda x: tf.add(b, x))\n",
    "\n",
    "res = multiply_layer(x_data)\n",
    "outputs = add_layer(res)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs=x_data, outputs=outputs, name=\"gate_2\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer=tf.keras.optimizers.SGD(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing two Gate Output to 50\n",
      "Step: 0 ==> 5.400000095367432 * 5.0 + 1.8799999952316284 = 28.8799991607666\n",
      "Step: 1 ==> 7.51200008392334 * 5.0 + 2.3024001121520996 = 39.86240005493164\n",
      "Step: 2 ==> 8.52575969696045 * 5.0 + 2.5051522254943848 = 45.13395309448242\n",
      "Step: 3 ==> 9.012364387512207 * 5.0 + 2.602473258972168 = 47.6642951965332\n",
      "Step: 4 ==> 9.24593448638916 * 5.0 + 2.6491873264312744 = 48.87886047363281\n",
      "Step: 5 ==> 9.358048439025879 * 5.0 + 2.671610116958618 = 49.46185302734375\n",
      "Step: 6 ==> 9.411863327026367 * 5.0 + 2.682373046875 = 49.74169158935547\n",
      "Step: 7 ==> 9.437694549560547 * 5.0 + 2.6875391006469727 = 49.87601089477539\n",
      "Step: 8 ==> 9.450093269348145 * 5.0 + 2.690018892288208 = 49.94048309326172\n",
      "Step: 9 ==> 9.456045150756836 * 5.0 + 2.691209316253662 = 49.971435546875\n"
     ]
    }
   ],
   "source": [
    "\"\"\" We now optimize the model variables to train the output toward the target value of 50, shown as follows:\"\"\"\n",
    "print('Optimizing two Gate Output to 50')\n",
    "for i in range(10):\n",
    "    \n",
    "    # Open a GradientTape\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        \n",
    "        # Forward pass.\n",
    "        two_gate_output = model(x_val)\n",
    "        \n",
    "        # Loss value as the difference between the output and a target value, 50.\n",
    "        loss_value = tf.square(tf.subtract(two_gate_output, 50.))\n",
    "        \n",
    "    # Get gradients of loss with reference to the variables \"a\" and \"b\" to adjust\n",
    "    gradients_a = tape.gradient(loss_value, a)\n",
    "    gradients_b = tape.gradient(loss_value, b)\n",
    "    \n",
    "    # Update the variables \"a\" and \"b\" of the model\n",
    "    optimizer.apply_gradients(zip([gradients_a, gradients_b], [a, b]))\n",
    "    \n",
    "    print(f'Step: {i} ==> {a.numpy()} * {x_val} + {b.numpy()} = {a.numpy()*x_val+b}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py365",
   "language": "python",
   "name": "py365"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
