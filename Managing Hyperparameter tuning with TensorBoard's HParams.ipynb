{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ready..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how the HParams plugin works, we will use a sequential model implementation on the MNIST dataset. We'll configure HParams and compare several hyperparameter combinations in order to find the best hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load in the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load and prepare the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Set model parameters\n",
    "image_width = x_train[0].shape[0]\n",
    "image_height = x_train[0].shape[1]\n",
    "num_channels = 1 # Greyscale = 1 channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For each hyperparameter, we define the list or the interval of values to test. In this section, we'll go over three hyperparameters: the number of units per layer, the dropout rate, and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_ARCHITECTURE_NN = hp.HParam('archi_nn', hp.Discrete(['128, 64', '256, 128']))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.1))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We now define our model to be able to test the different hyperparameter combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(hparams, experiment_run_log_dir):\n",
    "    \n",
    "    nb_units = list(map(int, hparams[HP_ARCHITECTURE_NN].split(\",\")))\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(name='FLATTEN'))\n",
    "    model.add(tf.keras.layers.Dense(units=nb_units[0], activation='relu', name='D1'))\n",
    "    model.add(tf.keras.layers.Dropout(hparams[HP_DROPOUT], name='DROP_OUT'))\n",
    "    model.add(tf.keras.layers.Dense(units=nb_units[1], activation='relu', name='D2'))\n",
    "    model.add(tf.keras.layers.Dense(units=10, activation='softmax', name='OUTPUT'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=hparams[HP_OPTIMIZER],\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=experiment_run_log_dir)\n",
    "    hparams_callback = hp.KerasCallback(experiment_run_log_dir, hparams)\n",
    "    \n",
    "    model.fit(x=x_train,\n",
    "              y=y_train,\n",
    "              epochs=5,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks=[tensorboard_callback, hparams_callback]\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now we iterate over the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2351 - accuracy: 0.9315 - val_loss: 0.1292 - val_accuracy: 0.9611\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 916us/step - loss: 0.0994 - accuracy: 0.9700 - val_loss: 0.0897 - val_accuracy: 0.9695\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 919us/step - loss: 0.0701 - accuracy: 0.9778 - val_loss: 0.0896 - val_accuracy: 0.9729\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 908us/step - loss: 0.0543 - accuracy: 0.9832 - val_loss: 0.0817 - val_accuracy: 0.9752\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 922us/step - loss: 0.0424 - accuracy: 0.9865 - val_loss: 0.0921 - val_accuracy: 0.9738\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2605 - accuracy: 0.9237 - val_loss: 0.1180 - val_accuracy: 0.9648\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 942us/step - loss: 0.1186 - accuracy: 0.9638 - val_loss: 0.1000 - val_accuracy: 0.9668\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 939us/step - loss: 0.0879 - accuracy: 0.9729 - val_loss: 0.0893 - val_accuracy: 0.9719\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 951us/step - loss: 0.0707 - accuracy: 0.9772 - val_loss: 0.0814 - val_accuracy: 0.9741\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 933us/step - loss: 0.0587 - accuracy: 0.9812 - val_loss: 0.0768 - val_accuracy: 0.9767\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 995us/step - loss: 0.6332 - accuracy: 0.8303 - val_loss: 0.3142 - val_accuracy: 0.9105\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 863us/step - loss: 0.2854 - accuracy: 0.9179 - val_loss: 0.2476 - val_accuracy: 0.9268\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 852us/step - loss: 0.2390 - accuracy: 0.9312 - val_loss: 0.2125 - val_accuracy: 0.9379\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 865us/step - loss: 0.2082 - accuracy: 0.9404 - val_loss: 0.1900 - val_accuracy: 0.9427\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 860us/step - loss: 0.1854 - accuracy: 0.9459 - val_loss: 0.1754 - val_accuracy: 0.9471\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.7039 - accuracy: 0.8032 - val_loss: 0.3256 - val_accuracy: 0.9103\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 878us/step - loss: 0.3283 - accuracy: 0.9049 - val_loss: 0.2541 - val_accuracy: 0.9285\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 886us/step - loss: 0.2645 - accuracy: 0.9243 - val_loss: 0.2105 - val_accuracy: 0.9404\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 883us/step - loss: 0.2234 - accuracy: 0.9357 - val_loss: 0.1807 - val_accuracy: 0.9478\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 875us/step - loss: 0.1976 - accuracy: 0.9435 - val_loss: 0.1602 - val_accuracy: 0.9541\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2092 - accuracy: 0.9379 - val_loss: 0.1006 - val_accuracy: 0.9699\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 908us/step - loss: 0.0868 - accuracy: 0.9731 - val_loss: 0.0850 - val_accuracy: 0.9735\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 905us/step - loss: 0.0594 - accuracy: 0.9813 - val_loss: 0.0762 - val_accuracy: 0.9773\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 901us/step - loss: 0.0438 - accuracy: 0.9858 - val_loss: 0.0889 - val_accuracy: 0.9736\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 903us/step - loss: 0.0337 - accuracy: 0.9890 - val_loss: 0.0817 - val_accuracy: 0.9763\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2171 - accuracy: 0.9354 - val_loss: 0.1044 - val_accuracy: 0.9683\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 935us/step - loss: 0.0969 - accuracy: 0.9703 - val_loss: 0.0846 - val_accuracy: 0.9731\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 951us/step - loss: 0.0712 - accuracy: 0.9782 - val_loss: 0.0802 - val_accuracy: 0.9768\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 954us/step - loss: 0.0546 - accuracy: 0.9832 - val_loss: 0.0723 - val_accuracy: 0.9791\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 944us/step - loss: 0.0481 - accuracy: 0.9847 - val_loss: 0.0739 - val_accuracy: 0.9791\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5921 - accuracy: 0.8466 - val_loss: 0.2994 - val_accuracy: 0.9156\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 871us/step - loss: 0.2814 - accuracy: 0.9208 - val_loss: 0.2376 - val_accuracy: 0.9332\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 870us/step - loss: 0.2303 - accuracy: 0.9334 - val_loss: 0.2068 - val_accuracy: 0.9402\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 876us/step - loss: 0.1967 - accuracy: 0.9439 - val_loss: 0.1745 - val_accuracy: 0.9502\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 878us/step - loss: 0.1717 - accuracy: 0.9513 - val_loss: 0.1601 - val_accuracy: 0.9526\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.6225 - accuracy: 0.8342 - val_loss: 0.3000 - val_accuracy: 0.9158\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 906us/step - loss: 0.3018 - accuracy: 0.9129 - val_loss: 0.2360 - val_accuracy: 0.9339\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 893us/step - loss: 0.2439 - accuracy: 0.9294 - val_loss: 0.1995 - val_accuracy: 0.9422\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 894us/step - loss: 0.2067 - accuracy: 0.9398 - val_loss: 0.1725 - val_accuracy: 0.9502\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 894us/step - loss: 0.1799 - accuracy: 0.9484 - val_loss: 0.1543 - val_accuracy: 0.9547\n"
     ]
    }
   ],
   "source": [
    "for archi_nn in HP_ARCHITECTURE_NN.domain.values:\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "        for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "            hparams = {\n",
    "                HP_ARCHITECTURE_NN : archi_nn,\n",
    "                HP_OPTIMIZER : optimizer,\n",
    "                HP_DROPOUT : dropout_rate\n",
    "            }\n",
    "            \n",
    "            experiment_run_log_dir='logs/experiment-' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            \n",
    "            train_model(hparams, experiment_run_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. And we'll start the TensorBoard application..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e9d1185c429ed4f0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e9d1185c429ed4f0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir='logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above interactive application lets us visualize and review the performance of our different models based on the hyperparameter sweep. Going to 'HPARAMS' -> 'TABLE VIEW' demonstrates the best performance was achieved with 0.1 dropout, 256, 128 nodes, and adam optimizer. The 'PARALLEL COORDINATES VIEW' enables a great visual comparison and quick determination of best performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py365",
   "language": "python",
   "name": "py365"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
